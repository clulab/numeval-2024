{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa9f64e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-19 19:33:30.666419: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-11-19 19:33:30.803710: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-11-19 19:33:30.804820: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-19 19:33:38.649060: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer\n",
    "import re\n",
    "from transformers import AdamWeightDecay\n",
    "from datasets import Dataset\n",
    "import math\n",
    "import numpy as np\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "from transformers import TFAutoModelForSeq2SeqLM\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import evaluate\n",
    "from transformers import T5Tokenizer, T5Config, T5ForConditionalGeneration\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9828453",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def json_to_dataframe(filename):\n",
    "    f = open(filename)\n",
    "    df = pd.read_json(f)\n",
    "    print(df.iloc[0])\n",
    "    f.close()\n",
    "    df[\"masked headline\"]= df.apply(lambda x: x['masked headline'].replace('____', str(x['ans'])), axis=1)\n",
    "    print(df.iloc[0])\n",
    "    df['news'] = df['news'].apply(lambda x: re.sub(r'\\([^)]*\\)', '', x))\n",
    "    #df['masked headline'] = df['masked headline'].str.replace('____', '<extra_id_0> ') + ' </s>'\n",
    "    df['text'] = df[['news', 'masked headline']].apply(\" \".join, axis=1)\n",
    "    df = df.astype(str)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08982d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "news               (Oct 7, 2014  12:40 PM CDT) As of Jan. 1, Walm...\n",
      "masked headline    ____K Walmart Part-Timers to Lose Health Insur...\n",
      "calculation                                     Paraphrase(30,000,K)\n",
      "ans                                                               30\n",
      "Name: 0, dtype: object\n",
      "news               (Oct 7, 2014  12:40 PM CDT) As of Jan. 1, Walm...\n",
      "masked headline     30K Walmart Part-Timers to Lose Health Insurance\n",
      "calculation                                     Paraphrase(30,000,K)\n",
      "ans                                                               30\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "train_data = json_to_dataframe('Train_Numerical_Reasoning.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82e5eceb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news</th>\n",
       "      <th>masked headline</th>\n",
       "      <th>calculation</th>\n",
       "      <th>ans</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>As of Jan. 1, Walmart will no longer offer 30...</td>\n",
       "      <td>30K Walmart Part-Timers to Lose Health Insurance</td>\n",
       "      <td>Paraphrase(30,000,K)</td>\n",
       "      <td>30</td>\n",
       "      <td>As of Jan. 1, Walmart will no longer offer 30...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dax Shepard and Kristen Bell got married at t...</td>\n",
       "      <td>Dax Shepard: Wedding to Kristen Bell Cost $142</td>\n",
       "      <td>Copy(142)</td>\n",
       "      <td>142</td>\n",
       "      <td>Dax Shepard and Kristen Bell got married at t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nancy Reagan, the helpmate, backstage adviser...</td>\n",
       "      <td>Nancy Reagan Dead at 94</td>\n",
       "      <td>Copy(94)</td>\n",
       "      <td>94</td>\n",
       "      <td>Nancy Reagan, the helpmate, backstage adviser...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>American Airlines faces FAA fines of more tha...</td>\n",
       "      <td>American Airlines Faces $7M Fine for Safety Vi...</td>\n",
       "      <td>Copy(7)</td>\n",
       "      <td>7</td>\n",
       "      <td>American Airlines faces FAA fines of more tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ingrid Lyne, the Seattle mom allegedly murder...</td>\n",
       "      <td>$222K Raised for Kids of Mom Dismembered on Date</td>\n",
       "      <td>Paraphrase(222,000,K)</td>\n",
       "      <td>222</td>\n",
       "      <td>Ingrid Lyne, the Seattle mom allegedly murder...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                news  \\\n",
       "0   As of Jan. 1, Walmart will no longer offer 30...   \n",
       "1   Dax Shepard and Kristen Bell got married at t...   \n",
       "2   Nancy Reagan, the helpmate, backstage adviser...   \n",
       "3   American Airlines faces FAA fines of more tha...   \n",
       "4   Ingrid Lyne, the Seattle mom allegedly murder...   \n",
       "\n",
       "                                     masked headline            calculation  \\\n",
       "0   30K Walmart Part-Timers to Lose Health Insurance   Paraphrase(30,000,K)   \n",
       "1     Dax Shepard: Wedding to Kristen Bell Cost $142              Copy(142)   \n",
       "2                            Nancy Reagan Dead at 94               Copy(94)   \n",
       "3  American Airlines Faces $7M Fine for Safety Vi...                Copy(7)   \n",
       "4   $222K Raised for Kids of Mom Dismembered on Date  Paraphrase(222,000,K)   \n",
       "\n",
       "   ans                                               text  \n",
       "0   30   As of Jan. 1, Walmart will no longer offer 30...  \n",
       "1  142   Dax Shepard and Kristen Bell got married at t...  \n",
       "2   94   Nancy Reagan, the helpmate, backstage adviser...  \n",
       "3    7   American Airlines faces FAA fines of more tha...  \n",
       "4  222   Ingrid Lyne, the Seattle mom allegedly murder...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e162d496",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_pandas(train_data)\n",
    "dataset = dataset.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6dc9a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_name = \"ybagoury/flan-t5-base-tldr_news\"\n",
    "#model_name = \"Michau/t5-base-en-generate-headline\"\n",
    "model_name = 'MBZUAI/LaMini-Flan-T5-783M'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "affdc701",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'MBZUAI/LaMini-Flan-T5-783M'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "optimizer = AdamWeightDecay(learning_rate=5e-5, weight_decay_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5277d12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c352b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer,\n",
    "                                           model=model_name,\n",
    "                                           return_tensors=\"tf\")\n",
    "#optimizer = AdamWeightDecay(learning_rate=1e-4, weight_decay_rate=0.01)\n",
    "#optimizer = AdamWeightDecay(learning_rate=7e-5, weight_decay_rate=0.01)\n",
    "optimizer = AdamWeightDecay(learning_rate=5e-5, weight_decay_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f347345",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function2(examples):   \n",
    "    inputs = tokenizer(examples['calculation'],max_length=100,  truncation=True)\n",
    "    #labels = tokenizer(text_target=examples[\"headline\"], max_length=128, truncation=True)\n",
    "    labels = tokenizer(text_target=examples[\"ans\"], max_length=20, truncation=True)\n",
    "    inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c6d5ae8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28dcafea6ee84960b6173e6c468e0eac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/16925 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffc1df2fe09d4a6ea0bc82f48cce214a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4232 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized2 = dataset.map(preprocess_function2, batched=True, #num_proc=4,\n",
    "                        remove_columns=dataset['train'].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6cceb4c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "tf_train_set2 = model2.prepare_tf_dataset(\n",
    "    tokenized2[\"train\"],\n",
    "    shuffle=True,\n",
    "    batch_size=3,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "\n",
    "tf_test_set2 = model2.prepare_tf_dataset(\n",
    "    tokenized2[\"test\"],\n",
    "    shuffle=False,\n",
    "    batch_size=3,\n",
    "    collate_fn=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8bea6bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-18 13:42:23.252142: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15520 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:0b:00.0, compute capability: 6.0\n",
      "/home/u3/hinokicrum/.local/lib/python3.9/site-packages/keras/src/initializers/initializers.py:120: UserWarning: The initializer RandomNormal is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
      "  warnings.warn(\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFT5ForConditionalGeneration: ['decoder.embed_tokens.weight', 'encoder.embed_tokens.weight']\n",
      "- This IS expected if you are initializing TFT5ForConditionalGeneration from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFT5ForConditionalGeneration from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model2 = TFAutoModelForSeq2SeqLM.from_pretrained(model_name, from_pt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6d70dad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "5641/5641 [==============================] - 1471s 248ms/step - loss: 0.1828 - val_loss: 0.1106\n",
      "Epoch 2/8\n",
      "5641/5641 [==============================] - 1380s 245ms/step - loss: 0.1145 - val_loss: 0.1065\n",
      "Epoch 3/8\n",
      "5641/5641 [==============================] - 1376s 244ms/step - loss: 0.0902 - val_loss: 0.1037\n",
      "Epoch 4/8\n",
      "5641/5641 [==============================] - 1377s 244ms/step - loss: 0.0789 - val_loss: 0.1099\n"
     ]
    }
   ],
   "source": [
    "model2.compile(optimizer=optimizer)\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=1)\n",
    "history2 = model2.fit(x=tf_train_set2, validation_data=tf_test_set2, callbacks=[callback], epochs=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3bdbfb3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/u3/hinokicrum/.local/lib/python3.9/site-packages/transformers/generation/tf_utils.py:465: UserWarning: `seed_generator` is deprecated and will be removed in a future version.\n",
      "  warnings.warn(\"`seed_generator` is deprecated and will be removed in a future version.\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "model2.save('Lamini182.keras')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "314aafbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-19 19:34:09.209407: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "/home/u3/hinokicrum/.local/lib/python3.8/site-packages/keras/src/initializers/initializers.py:120: UserWarning: The initializer RandomNormal is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`compile()` was not called as part of model loading because the model's `compile()` method is custom. All subclassed Models that have `compile()` overridden should also override `get_compile_config()` and `compile_from_config(config)`. Alternatively, you can call `compile()` manually after loading.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/u3/hinokicrum/.local/lib/python3.8/site-packages/transformers/generation/tf_utils.py:465: UserWarning: `seed_generator` is deprecated and will be removed in a future version.\n",
      "  warnings.warn(\"`seed_generator` is deprecated and will be removed in a future version.\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "model2 = tf.keras.models.load_model('Lamini182.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bfe318a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('Dev_Numerical_Reasoning.json')\n",
    "df = pd.read_json(f)\n",
    "df['news'] = df['news'].apply(lambda x: re.sub(r'\\([^)]*\\)', '', x))\n",
    "df['masked headline'] = df['masked headline'].str.replace('____', '<extra_id_0> ') + ' </s>'\n",
    "df['text'] = df[['news', 'masked headline']].apply(\" \".join, axis=1)\n",
    "df = df.astype(str)\n",
    "#df = df.iloc[:200]\n",
    "#df = df.iloc[100:500]\n",
    "f.close()\n",
    "test_data = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "918ae06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cal = pd.read_excel('Lamini3.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4feae44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def t5_predictions2(data, tokenizer, model):\n",
    "\n",
    "\n",
    "    inputs = tokenizer.encode_plus(data, add_special_tokens=True,\n",
    "                                   return_tensors=\"pt\").input_ids\n",
    "    # input_ids = inputs['input_ids'].to(device)\n",
    "    # attention_masks = inputs['attention_mask']\n",
    "\n",
    "    outputs = model.generate(inputs,\n",
    "                             max_length=128, num_beams=5, early_stopping=True)\n",
    "\n",
    "\n",
    "    res = ''\n",
    "    results = tokenizer.decode(outputs[0][1:], skip_special_tokens=False,\n",
    "                               clean_up_tokenization_spaces=False)\n",
    "    res = re.findall(r\"([\\d:,./]+)\", results)\n",
    "    return res[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "05eab8e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1853\n",
      "12\n",
      "\n",
      "200\n",
      "4\n",
      "\n",
      "1\n",
      "2\n",
      "\n",
      "2010\n",
      "2000\n",
      "\n",
      "900\n",
      "2\n",
      "\n",
      "1\n",
      "2\n",
      "\n",
      "16\n",
      "2\n",
      "\n",
      "8\n",
      "40\n",
      "\n",
      "2005\n",
      "2.9\n",
      "\n",
      "100\n",
      "1\n",
      "\n",
      "15\n",
      "59\n",
      "\n",
      "10\n",
      "9\n",
      "\n",
      "3\n",
      "2.8\n",
      "\n",
      "4\n",
      "4.5\n",
      "\n",
      "300\n",
      "350\n",
      "\n",
      "1.5\n",
      "1\n",
      "\n",
      "37\n",
      "31\n",
      "\n",
      "1\n",
      "100\n",
      "\n",
      "43\n",
      "44\n",
      "\n",
      "118\n",
      "2\n",
      "\n",
      "100\n",
      "3\n",
      "\n",
      "1811\n",
      "1892\n",
      "\n",
      "10\n",
      "17\n",
      "\n",
      "1972\n",
      "3\n",
      "\n",
      "24\n",
      "26\n",
      "\n",
      "223\n",
      "62\n",
      "\n",
      "9/11\n",
      "100\n",
      "\n",
      "74\n",
      "2\n",
      "\n",
      "4\n",
      "3.2\n",
      "\n",
      "110\n",
      "100\n",
      "\n",
      "25\n",
      "27\n",
      "\n",
      "200\n",
      "800\n",
      "\n",
      "69\n",
      "100\n",
      "\n",
      "26.6\n",
      "28.6\n",
      "\n",
      "633\n",
      "643\n",
      "\n",
      "4\n",
      "3\n",
      "\n",
      "17\n",
      "2\n",
      "\n",
      "160\n",
      "16\n",
      "\n",
      "219\n",
      "230\n",
      "\n",
      "12\n",
      "3\n",
      "\n",
      "1677\n",
      "1667\n",
      "\n",
      "1\n",
      "15\n",
      "\n",
      "2\n",
      "1\n",
      "\n",
      "47\n",
      "2\n",
      "\n",
      "513\n",
      "500\n",
      "\n",
      "2009\n",
      "1981\n",
      "\n",
      "58\n",
      "90\n",
      "\n",
      "2\n",
      "02\n",
      "\n",
      "32\n",
      "60\n",
      "\n",
      "2\n",
      "12\n",
      "\n",
      "6\n",
      "1.2\n",
      "\n",
      "877\n",
      "20\n",
      "\n",
      "4\n",
      "200\n",
      "\n",
      "10\n",
      "2005\n",
      "\n",
      "14\n",
      "12\n",
      "\n",
      "20\n",
      "5\n",
      "\n",
      "4\n",
      "1\n",
      "\n",
      "26\n",
      "25\n",
      "\n",
      "27\n",
      "2\n",
      "\n",
      "2\n",
      "0\n",
      "\n",
      "4\n",
      "3\n",
      "\n",
      "5\n",
      "7\n",
      "\n",
      "35\n",
      "12\n",
      "\n",
      "185\n",
      "32\n",
      "\n",
      "10\n",
      "2\n",
      "\n",
      "62\n",
      "38\n",
      "\n",
      "76\n",
      "72\n",
      "\n",
      "3\n",
      "2\n",
      "\n",
      "8\n",
      "1\n",
      "\n",
      "10\n",
      "31\n",
      "\n",
      "24\n",
      "12\n",
      "\n",
      "98\n",
      "9.8\n",
      "\n",
      "1\n",
      "7\n",
      "\n",
      "30\n",
      "73\n",
      "\n",
      "5\n",
      "100\n",
      "\n",
      "23,334\n",
      "23,344\n",
      "\n",
      "4\n",
      "100\n",
      "\n",
      "50\n",
      "30\n",
      "\n",
      "2.7\n",
      "2.77\n",
      "\n",
      "1963\n",
      "1958\n",
      "\n",
      "2009\n",
      "70\n",
      "\n",
      "73\n",
      "5\n",
      "\n",
      "4.8\n",
      "5\n",
      "\n",
      "25\n",
      "160\n",
      "\n",
      "1\n",
      "3\n",
      "\n",
      "258\n",
      "248\n",
      "\n",
      "100\n",
      "2\n",
      "\n",
      "3\n",
      "3.5\n",
      "\n",
      "88\n",
      "89\n",
      "\n",
      "11\n",
      "192\n",
      "\n",
      "3\n",
      "7\n",
      "\n",
      "359\n",
      "1\n",
      "\n",
      "204\n",
      "205\n",
      "\n",
      "60\n",
      "70\n",
      "\n",
      "10\n",
      "3\n",
      "\n",
      "6\n",
      "2\n",
      "\n",
      "1\n",
      "99\n",
      "\n",
      "1,08\n",
      "1,018\n",
      "\n",
      "24\n",
      "83\n",
      "\n",
      "20\n",
      "2\n",
      "\n",
      "1\n",
      "19,000\n",
      "\n",
      "8\n",
      "221\n",
      "\n",
      "3\n",
      "94\n",
      "\n",
      "10\n",
      "11\n",
      "\n",
      "40\n",
      "2\n",
      "\n",
      "1991\n",
      "1990\n",
      "\n",
      "44\n",
      "45\n",
      "\n",
      "1\n",
      "136\n",
      "\n",
      "5\n",
      "1.5\n",
      "\n",
      "8\n",
      "9\n",
      "\n",
      "2035\n",
      "617\n",
      "\n",
      "30\n",
      "8\n",
      "\n",
      "33\n",
      "30\n",
      "\n",
      "4\n",
      "2\n",
      "\n",
      "40\n",
      "82\n",
      "\n",
      "630\n",
      "730\n",
      "\n",
      "3\n",
      "5\n",
      "\n",
      "1.2\n",
      "11\n",
      "\n",
      "10.5\n",
      "10.10\n",
      "\n",
      "151\n",
      "88\n",
      "\n",
      "1.8\n",
      "1.9\n",
      "\n",
      "2014\n",
      "16\n",
      "\n",
      "40.7\n",
      "48.7\n",
      "\n",
      "83\n",
      "09\n",
      "\n",
      "290\n",
      "390\n",
      "\n",
      "3\n",
      "2\n",
      "\n",
      "130\n",
      "4\n",
      "\n",
      "300\n",
      "299\n",
      "\n",
      "15\n",
      "14\n",
      "\n",
      "100\n",
      "9\n",
      "\n",
      "83\n",
      "100\n",
      "\n",
      "480\n",
      "500\n",
      "\n",
      "5\n",
      "6\n",
      "\n",
      "100\n",
      "60\n",
      "\n",
      "15\n",
      "11\n",
      "\n",
      "99\n",
      "0.99\n",
      "\n",
      "37\n",
      "39\n",
      "\n",
      "10\n",
      "30\n",
      "\n",
      "57\n",
      "2\n",
      "\n",
      "1808\n",
      "1200\n",
      "\n",
      "273\n",
      "270\n",
      "\n",
      "359\n",
      "459\n",
      "\n",
      "30\n",
      "60\n",
      "\n",
      "1\n",
      "2013\n",
      "\n",
      "50\n",
      "3\n",
      "\n",
      "2012\n",
      "2009\n",
      "\n",
      "9\n",
      "10\n",
      "\n",
      "3\n",
      "11\n",
      "\n",
      "5\n",
      "3\n",
      "\n",
      "10\n",
      "6\n",
      "\n",
      "40\n",
      "35\n",
      "\n",
      "1960\n",
      "67\n",
      "\n",
      "10\n",
      "11\n",
      "\n",
      "1\n",
      "6\n",
      "\n",
      "11\n",
      "5\n",
      "\n",
      "7\n",
      "9\n",
      "\n",
      "80,000\n",
      "1.9\n",
      "\n",
      "83\n",
      "84\n",
      "\n",
      "103\n",
      "100\n",
      "\n",
      "109\n",
      "3\n",
      "\n",
      "239\n",
      "7\n",
      "\n",
      "75\n",
      "25\n",
      "\n",
      "96\n",
      "99.5\n",
      "\n",
      "61\n",
      "2\n",
      "\n",
      "5\n",
      "13\n",
      "\n",
      "2\n",
      "11\n",
      "\n",
      "3.6\n",
      "3\n",
      "\n",
      "3\n",
      "1\n",
      "\n",
      "2010\n",
      "10\n",
      "\n",
      "150\n",
      "2\n",
      "\n",
      "2\n",
      "7\n",
      "\n",
      "9\n",
      "90\n",
      "\n",
      "6.7\n",
      "7.6\n",
      "\n",
      "29\n",
      "607\n",
      "\n",
      "90\n",
      "3\n",
      "\n",
      "37\n",
      "12\n",
      "\n",
      "11\n",
      "40\n",
      "\n",
      "240\n",
      "7\n",
      "\n",
      "70\n",
      "30\n",
      "\n",
      "71\n",
      "70\n",
      "\n",
      "3\n",
      "13\n",
      "\n",
      "2010\n",
      "2011\n",
      "\n",
      "31\n",
      "3\n",
      "\n",
      "25\n",
      "21\n",
      "\n",
      "152\n",
      "162\n",
      "\n",
      "77\n",
      "6.3\n",
      "\n",
      "4.9\n",
      "92\n",
      "\n",
      "4\n",
      "2\n",
      "\n",
      "1\n",
      "50\n",
      "\n",
      "1.2\n",
      "1\n",
      "\n",
      "13\n",
      "30\n",
      "\n",
      "10\n",
      "23\n",
      "\n",
      "2014\n",
      "5\n",
      "\n",
      "60\n",
      "3\n",
      "\n",
      "100\n",
      "1\n",
      "\n",
      "5\n",
      "0\n",
      "\n",
      "860\n",
      "1\n",
      "\n",
      "5\n",
      "1\n",
      "\n",
      "1\n",
      "3\n",
      "\n",
      "2012\n",
      "2\n",
      "\n",
      "3,750\n",
      "37.50\n",
      "\n",
      "12\n",
      "97\n",
      "\n",
      "4\n",
      "9\n",
      "\n",
      "74\n",
      "75\n",
      "\n",
      "239\n",
      "240\n",
      "\n",
      "100\n",
      "11\n",
      "\n",
      "152\n",
      "162\n",
      "\n",
      "1\n",
      "24\n",
      "\n",
      "12\n",
      "266\n",
      "\n",
      "2012\n",
      "2011\n",
      "\n",
      "2\n",
      "1\n",
      "\n",
      "10.5\n",
      "10.10\n",
      "\n",
      "60\n",
      "30\n",
      "\n",
      "2\n",
      "8\n",
      "\n",
      "132\n",
      "3\n",
      "\n",
      "20\n",
      "2.13\n",
      "\n",
      "3\n",
      "2\n",
      "\n",
      "5\n",
      "9/11\n",
      "\n",
      "4\n",
      "2\n",
      "\n",
      "2015\n",
      "16\n",
      "\n",
      "25\n",
      "2,500\n",
      "\n",
      "1200\n",
      "400\n",
      "\n",
      "151\n",
      "216\n",
      "\n",
      "90\n",
      "2\n",
      "\n",
      "1\n",
      "2\n",
      "\n",
      "18\n",
      "43\n",
      "\n",
      "6\n",
      "5\n",
      "\n",
      "1\n",
      "6\n",
      "\n",
      "5\n",
      "2\n",
      "\n",
      "4\n",
      "1968\n",
      "\n",
      "136\n",
      "6\n",
      "\n",
      "8\n",
      "737\n",
      "\n",
      "78\n",
      "7.8\n",
      "\n",
      "5\n",
      "332\n",
      "\n",
      "200\n",
      "11\n",
      "\n"
     ]
    }
   ],
   "source": [
    "false=0\n",
    "predictions = []\n",
    "for x,y in zip(cal['calc'], test_data['ans']):\n",
    "    ans = t5_predictions2(x, tokenizer, model2)\n",
    "    predictions.append(ans)\n",
    "    if ans != y:\n",
    "        print(ans)\n",
    "        print(y)\n",
    "        print(\"\")\n",
    "        false += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "40e525a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "235"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "370fe76b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9086314152410575\n"
     ]
    }
   ],
   "source": [
    "print(1-false/len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ea9c7ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sub1_predictions.txt', 'w+') as f:\n",
    "     \n",
    "    for prediction in predictions:\n",
    "        f.write('%s\\n' %prediction)\n",
    "     \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567a2a69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
