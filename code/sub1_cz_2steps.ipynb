{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa9f64e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-21 09:44:25.576254: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-21 09:44:29.211321: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer\n",
    "import re\n",
    "from transformers import AdamWeightDecay\n",
    "from datasets import Dataset\n",
    "import math\n",
    "import numpy as np\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "from transformers import TFAutoModelForSeq2SeqLM\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import evaluate\n",
    "from transformers import T5Tokenizer, T5Config, T5ForConditionalGeneration\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a66757e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.experimental.list_physical_devices(device_type='GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9828453",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def json_to_dataframe(filename):\n",
    "    f = open(filename)\n",
    "    df = pd.read_json(f)\n",
    "    print(df.iloc[0])\n",
    "    f.close()\n",
    "    #df[\"maskedd headline\"]= df.apply(lambda x: x['masked headline'].replace('____', str(x['ans'])), axis=1)\n",
    "    print(df.iloc[0])\n",
    "    df['news'] = df['news'].apply(lambda x: re.sub(r'\\([^)]*\\)', '', x))\n",
    "    df['masked headline'] = df['masked headline'].str.replace('____', '<extra_id_0> ') + ' </s>'\n",
    "    df['text'] = df[['news', 'masked headline']].apply(\" \".join, axis=1)\n",
    "    df = df.astype(str)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08982d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "news               (Oct 7, 2014  12:40 PM CDT) As of Jan. 1, Walm...\n",
      "masked headline    ____K Walmart Part-Timers to Lose Health Insur...\n",
      "calculation                                     Paraphrase(30,000,K)\n",
      "ans                                                               30\n",
      "Name: 0, dtype: object\n",
      "news               (Oct 7, 2014  12:40 PM CDT) As of Jan. 1, Walm...\n",
      "masked headline    ____K Walmart Part-Timers to Lose Health Insur...\n",
      "calculation                                     Paraphrase(30,000,K)\n",
      "ans                                                               30\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "train_data = json_to_dataframe('Train_Numerical_Reasoning.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6edde1b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news</th>\n",
       "      <th>masked headline</th>\n",
       "      <th>calculation</th>\n",
       "      <th>ans</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3513</th>\n",
       "      <td>A Florida man has won a $650,000 settlement f...</td>\n",
       "      <td>Man Gets $&lt;extra_id_0&gt; K After Stripper Kick &lt;/s&gt;</td>\n",
       "      <td>Paraphrase(650,000,K)</td>\n",
       "      <td>650</td>\n",
       "      <td>A Florida man has won a $650,000 settlement f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   news  \\\n",
       "3513   A Florida man has won a $650,000 settlement f...   \n",
       "\n",
       "                                        masked headline  \\\n",
       "3513  Man Gets $<extra_id_0> K After Stripper Kick </s>   \n",
       "\n",
       "                calculation  ans  \\\n",
       "3513  Paraphrase(650,000,K)  650   \n",
       "\n",
       "                                                   text  \n",
       "3513   A Florida man has won a $650,000 settlement f...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.loc[train_data['calculation']==\"Paraphrase(650,000,K)\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82e5eceb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news</th>\n",
       "      <th>masked headline</th>\n",
       "      <th>calculation</th>\n",
       "      <th>ans</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>As of Jan. 1, Walmart will no longer offer 30...</td>\n",
       "      <td>&lt;extra_id_0&gt; K Walmart Part-Timers to Lose Hea...</td>\n",
       "      <td>Paraphrase(30,000,K)</td>\n",
       "      <td>30</td>\n",
       "      <td>As of Jan. 1, Walmart will no longer offer 30...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dax Shepard and Kristen Bell got married at t...</td>\n",
       "      <td>Dax Shepard: Wedding to Kristen Bell Cost $&lt;ex...</td>\n",
       "      <td>Copy(142)</td>\n",
       "      <td>142</td>\n",
       "      <td>Dax Shepard and Kristen Bell got married at t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nancy Reagan, the helpmate, backstage adviser...</td>\n",
       "      <td>Nancy Reagan Dead at &lt;extra_id_0&gt;  &lt;/s&gt;</td>\n",
       "      <td>Copy(94)</td>\n",
       "      <td>94</td>\n",
       "      <td>Nancy Reagan, the helpmate, backstage adviser...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>American Airlines faces FAA fines of more tha...</td>\n",
       "      <td>American Airlines Faces $&lt;extra_id_0&gt; M Fine f...</td>\n",
       "      <td>Copy(7)</td>\n",
       "      <td>7</td>\n",
       "      <td>American Airlines faces FAA fines of more tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ingrid Lyne, the Seattle mom allegedly murder...</td>\n",
       "      <td>$&lt;extra_id_0&gt; K Raised for Kids of Mom Dismemb...</td>\n",
       "      <td>Paraphrase(222,000,K)</td>\n",
       "      <td>222</td>\n",
       "      <td>Ingrid Lyne, the Seattle mom allegedly murder...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                news  \\\n",
       "0   As of Jan. 1, Walmart will no longer offer 30...   \n",
       "1   Dax Shepard and Kristen Bell got married at t...   \n",
       "2   Nancy Reagan, the helpmate, backstage adviser...   \n",
       "3   American Airlines faces FAA fines of more tha...   \n",
       "4   Ingrid Lyne, the Seattle mom allegedly murder...   \n",
       "\n",
       "                                     masked headline            calculation  \\\n",
       "0  <extra_id_0> K Walmart Part-Timers to Lose Hea...   Paraphrase(30,000,K)   \n",
       "1  Dax Shepard: Wedding to Kristen Bell Cost $<ex...              Copy(142)   \n",
       "2            Nancy Reagan Dead at <extra_id_0>  </s>               Copy(94)   \n",
       "3  American Airlines Faces $<extra_id_0> M Fine f...                Copy(7)   \n",
       "4  $<extra_id_0> K Raised for Kids of Mom Dismemb...  Paraphrase(222,000,K)   \n",
       "\n",
       "   ans                                               text  \n",
       "0   30   As of Jan. 1, Walmart will no longer offer 30...  \n",
       "1  142   Dax Shepard and Kristen Bell got married at t...  \n",
       "2   94   Nancy Reagan, the helpmate, backstage adviser...  \n",
       "3    7   American Airlines faces FAA fines of more tha...  \n",
       "4  222   Ingrid Lyne, the Seattle mom allegedly murder...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e162d496",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_pandas(train_data)\n",
    "dataset = dataset.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6dc9a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name =\"czearing/article-title-generator\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1fcb4671",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):   \n",
    "    inputs = tokenizer(examples['text'],max_length=3066,  truncation=True)\n",
    "    labels = tokenizer(text_target=examples[\"calculation\"], max_length=128, truncation=True)\n",
    "    inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5277d12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d58f3fd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e43956b1aedc4016819b97abad756908",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/16925 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb202bfd22d04c6c9c32322796ca1e57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/4232 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized = dataset.map(preprocess_function, batched=True, num_proc=4,\n",
    "                        remove_columns=dataset['train'].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c352b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer,\n",
    "                                           model=model_name,\n",
    "                                           return_tensors=\"tf\")\n",
    "#optimizer = AdamWeightDecay(learning_rate=1e-4, weight_decay_rate=0.01)\n",
    "#optimizer = AdamWeightDecay(learning_rate=7e-5, weight_decay_rate=0.01)\n",
    "optimizer = AdamWeightDecay(learning_rate=5e-5, weight_decay_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5770ed7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-21 09:46:03.127950: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 31141 MB memory:  -> device: 0, name: Tesla V100S-PCIE-32GB, pci bus id: 0000:01:00.0, compute capability: 7.0\n",
      "2023-11-21 09:46:03.128857: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 31141 MB memory:  -> device: 1, name: Tesla V100S-PCIE-32GB, pci bus id: 0000:c1:00.0, compute capability: 7.0\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFT5ForConditionalGeneration: ['decoder.embed_tokens.weight', 'lm_head.weight', 'encoder.embed_tokens.weight']\n",
      "- This IS expected if you are initializing TFT5ForConditionalGeneration from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFT5ForConditionalGeneration from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = TFAutoModelForSeq2SeqLM.from_pretrained(model_name, from_pt=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182600a5",
   "metadata": {},
   "source": [
    "To convert datasets to the tf.data.Dataset format "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aeb30fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "tf_train_set = model.prepare_tf_dataset(\n",
    "    tokenized[\"train\"],\n",
    "    shuffle=True,\n",
    "    batch_size=16,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "\n",
    "tf_test_set = model.prepare_tf_dataset(\n",
    "    tokenized[\"test\"],\n",
    "    shuffle=False,\n",
    "    batch_size=16,\n",
    "    collate_fn=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "260c07c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d849f8d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1057/1057 [==============================] - 656s 596ms/step - loss: 0.6592 - val_loss: 0.3283\n",
      "Epoch 2/10\n",
      "1057/1057 [==============================] - 623s 589ms/step - loss: 0.3152 - val_loss: 0.2838\n",
      "Epoch 3/10\n",
      "1057/1057 [==============================] - 622s 588ms/step - loss: 0.2518 - val_loss: 0.2738\n",
      "Epoch 4/10\n",
      "1057/1057 [==============================] - 623s 589ms/step - loss: 0.2128 - val_loss: 0.2655\n",
      "Epoch 5/10\n",
      "1057/1057 [==============================] - 623s 589ms/step - loss: 0.1837 - val_loss: 0.2560\n",
      "Epoch 6/10\n",
      "1057/1057 [==============================] - 623s 590ms/step - loss: 0.1621 - val_loss: 0.2700\n"
     ]
    }
   ],
   "source": [
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=1)\n",
    "history = model.fit(x=tf_train_set, validation_data=tf_test_set, batch_size=6,callbacks=[callback], epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f9a65225",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function2(examples):   \n",
    "    inputs = tokenizer(examples['calculation'],max_length=400,  truncation=True)\n",
    "    #labels = tokenizer(text_target=examples[\"headline\"], max_length=128, truncation=True)\n",
    "    labels = tokenizer(text_target=examples[\"ans\"], max_length=50, truncation=True)\n",
    "    inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "54efab9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "937b19fd327d4cd5a4c0a5688b972899",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/16925 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8033ebc4028f48a6835ff86062ace351",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4232 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized2 = dataset.map(preprocess_function2, batched=True, #num_proc=4,\n",
    "                        remove_columns=dataset['train'].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b17e248f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFT5ForConditionalGeneration: ['decoder.embed_tokens.weight', 'lm_head.weight', 'encoder.embed_tokens.weight']\n",
      "- This IS expected if you are initializing TFT5ForConditionalGeneration from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFT5ForConditionalGeneration from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model2 = TFAutoModelForSeq2SeqLM.from_pretrained(model_name, from_pt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3b129ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_train_set2 = model2.prepare_tf_dataset(\n",
    "    tokenized2[\"train\"],\n",
    "    shuffle=True,\n",
    "    batch_size=16,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "\n",
    "tf_test_set2 = model2.prepare_tf_dataset(\n",
    "    tokenized2[\"test\"],\n",
    "    shuffle=False,\n",
    "    batch_size=16,\n",
    "    collate_fn=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2ab482ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1057/1057 [==============================] - 130s 101ms/step - loss: 0.2175 - val_loss: 0.1428\n",
      "Epoch 2/10\n",
      "1057/1057 [==============================] - 103s 97ms/step - loss: 0.1436 - val_loss: 0.1318\n",
      "Epoch 3/10\n",
      "1057/1057 [==============================] - 103s 97ms/step - loss: 0.1250 - val_loss: 0.1212\n",
      "Epoch 4/10\n",
      "1057/1057 [==============================] - 102s 96ms/step - loss: 0.1115 - val_loss: 0.1176\n",
      "Epoch 5/10\n",
      "1057/1057 [==============================] - 102s 96ms/step - loss: 0.1010 - val_loss: 0.1163\n",
      "Epoch 6/10\n",
      "1057/1057 [==============================] - 103s 97ms/step - loss: 0.0933 - val_loss: 0.1183\n"
     ]
    }
   ],
   "source": [
    "model2.compile(optimizer=optimizer)\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=1)\n",
    "history2 = model2.fit(x=tf_train_set2, validation_data=tf_test_set2,batch_size=6,callbacks=[callback], epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bfe318a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('Dev_Numerical_Reasoning.json')\n",
    "df = pd.read_json(f)\n",
    "df['news'] = df['news'].apply(lambda x: re.sub(r'\\([^)]*\\)', '', x))\n",
    "df['masked headline'] = df['masked headline'].str.replace('____', '<extra_id_0> ') + ' </s>'\n",
    "df['text'] = df[['news', 'masked headline']].apply(\" \".join, axis=1)\n",
    "df = df.astype(str)\n",
    "#df = df.iloc[:200]\n",
    "#df = df.iloc[100:500]\n",
    "f.close()\n",
    "test_data = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8dee4057",
   "metadata": {},
   "outputs": [],
   "source": [
    "def t5_predictions(data, tokenizer, model):\n",
    "    text = data['text']\n",
    "    headline = data['calculation']\n",
    "\n",
    "    inputs = tokenizer.encode_plus(text, add_special_tokens=True,\n",
    "                                   return_tensors=\"pt\").input_ids\n",
    "    outputs = model.generate(inputs,\n",
    "                             max_length=128, num_beams=5, early_stopping=True)\n",
    "\n",
    "    res = ''\n",
    "    results = tokenizer.decode(outputs[0][1:], skip_special_tokens=False,\n",
    "                               clean_up_tokenization_spaces=False)\n",
    "    return results        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0819156f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-21 10:57:45.152467: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fc13c01c810 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-11-21 10:57:45.152520: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100S-PCIE-32GB, Compute Capability 7.0\n",
      "2023-11-21 10:57:45.152526: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): Tesla V100S-PCIE-32GB, Compute Capability 7.0\n",
      "2023-11-21 10:57:45.156347: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-11-21 10:57:45.183636: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8902\n",
      "2023-11-21 10:57:45.190202: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:543] Can't find libdevice directory ${CUDA_DIR}/nvvm/libdevice. This may result in compilation or runtime failures, if the program we try to run uses routines from libdevice.\n",
      "Searched for CUDA in the following directories:\n",
      "  ./cuda_sdk_lib\n",
      "  /usr/local/cuda-11.8\n",
      "  /usr/local/cuda\n",
      "  .\n",
      "You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.\n",
      "2023-11-21 10:57:45.432437: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    }
   ],
   "source": [
    "cal = []\n",
    "for i in test_data:\n",
    "    cal.append(t5_predictions(i, tokenizer, model))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "077c40f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_cal = []\n",
    "for c in cal:\n",
    "    updated_cal.append(c.split('<')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d70b3d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def t5_predictions2(data, tokenizer, model):\n",
    "\n",
    "\n",
    "    inputs = tokenizer.encode_plus(data, add_special_tokens=True,\n",
    "                                   return_tensors=\"pt\").input_ids\n",
    "    outputs = model.generate(inputs,\n",
    "                             max_length=64, num_beams=5, early_stopping=True)\n",
    "\n",
    "\n",
    "    res = ''\n",
    "    results = tokenizer.decode(outputs[0][1:], skip_special_tokens=False,\n",
    "                               clean_up_tokenization_spaces=False)\n",
    "    res = re.findall(r\"([\\d:,./]+)\", results)\n",
    "    return res[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "370fe76b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "9\n",
      "\n",
      "1.29\n",
      "1\n",
      "\n",
      "1.1\n",
      "2\n",
      "\n",
      "2010\n",
      "2000\n",
      "\n",
      "3\n",
      "5\n",
      "\n",
      "2\n",
      "59\n",
      "\n",
      "4\n",
      "4.5\n",
      "\n",
      "120\n",
      "44\n",
      "\n",
      "700\n",
      "350\n",
      "\n",
      "30\n",
      "31\n",
      "\n",
      "2\n",
      "47\n",
      "\n",
      "79.5\n",
      "160\n",
      "\n",
      "70\n",
      "44\n",
      "\n",
      "1988\n",
      "21\n",
      "\n",
      "3\n",
      "29\n",
      "\n",
      "3\n",
      "4\n",
      "\n",
      "24\n",
      "85\n",
      "\n",
      "19\n",
      "1892\n",
      "\n",
      "12\n",
      "17\n",
      "\n",
      "1\n",
      "6\n",
      "\n",
      "3\n",
      "2\n",
      "\n",
      "2\n",
      "12\n",
      "\n",
      "4\n",
      "26\n",
      "\n",
      "91\n",
      "62\n",
      "\n",
      "3.3\n",
      "3\n",
      "\n",
      "8\n",
      "10\n",
      "\n",
      "10\n",
      "15.5\n",
      "\n",
      "109\n",
      "100\n",
      "\n",
      "4\n",
      "2\n",
      "\n",
      "5\n",
      "6\n",
      "\n",
      "25\n",
      "27\n",
      "\n",
      "200\n",
      "800\n",
      "\n",
      "29\n",
      "28.6\n",
      "\n",
      "12\n",
      "2\n",
      "\n",
      "3.1\n",
      "3\n",
      "\n",
      "300\n",
      "230\n",
      "\n",
      "1677\n",
      "1667\n",
      "\n",
      "5.2\n",
      "20\n",
      "\n",
      "3\n",
      "1\n",
      "\n",
      "1\n",
      "1.68\n",
      "\n",
      "7\n",
      "12\n",
      "\n",
      "513\n",
      "500\n",
      "\n",
      "90\n",
      "2010\n",
      "\n",
      "6\n",
      "51\n",
      "\n",
      "2\n",
      "3\n",
      "\n",
      "1\n",
      "1981\n",
      "\n",
      "950\n",
      "900\n",
      "\n",
      "169,000\n",
      "19\n",
      "\n",
      "10\n",
      "9\n",
      "\n",
      "30\n",
      "40\n",
      "\n",
      "10\n",
      "12\n",
      "\n",
      "12\n",
      "10\n",
      "\n",
      "47\n",
      "75\n",
      "\n",
      "5\n",
      "12\n",
      "\n",
      "6\n",
      "25\n",
      "\n",
      "11\n",
      "12\n",
      "\n",
      "40\n",
      "30\n",
      "\n",
      "5\n",
      "2\n",
      "\n",
      "2013\n",
      "7\n",
      "\n",
      "33\n",
      "30\n",
      "\n",
      "6\n",
      "136\n",
      "\n",
      "200000\n",
      "2\n",
      "\n",
      "18\n",
      "21\n",
      "\n",
      "177\n",
      "72\n",
      "\n",
      "100\n",
      "31\n",
      "\n",
      "10\n",
      "12\n",
      "\n",
      "9.5\n",
      "9.8\n",
      "\n",
      "28\n",
      "73\n",
      "\n",
      "27\n",
      "27.2\n",
      "\n",
      "10\n",
      "15\n",
      "\n",
      "50\n",
      "30\n",
      "\n",
      "50\n",
      "150\n",
      "\n",
      "23.3\n",
      "23\n",
      "\n",
      "100\n",
      "25\n",
      "\n",
      "23\n",
      "1958\n",
      "\n",
      "2012\n",
      "2011\n",
      "\n",
      "6\n",
      "5\n",
      "\n",
      "45\n",
      "50\n",
      "\n",
      "3\n",
      "5,000\n",
      "\n",
      "2009\n",
      "09\n",
      "\n",
      "8.5\n",
      "5\n",
      "\n",
      "2\n",
      "2014\n",
      "\n",
      "3\n",
      "2\n",
      "\n",
      "88\n",
      "89\n",
      "\n",
      "3\n",
      "32\n",
      "\n",
      "2\n",
      "1906\n",
      "\n",
      "2\n",
      "1\n",
      "\n",
      "12\n",
      "50\n",
      "\n",
      "4\n",
      "11\n",
      "\n",
      "204.6\n",
      "205\n",
      "\n",
      "1.5\n",
      "1\n",
      "\n",
      "50\n",
      "99\n",
      "\n",
      "25.5\n",
      "25\n",
      "\n",
      "200\n",
      "221\n",
      "\n",
      "47\n",
      "150\n",
      "\n",
      "7.7\n",
      "8\n",
      "\n",
      "10.9\n",
      "11\n",
      "\n",
      "187\n",
      "2\n",
      "\n",
      "6.2\n",
      "6\n",
      "\n",
      "6\n",
      "10\n",
      "\n",
      "1.11\n",
      "1.1\n",
      "\n",
      "56\n",
      "56.2\n",
      "\n",
      "44.7\n",
      "45\n",
      "\n",
      "40\n",
      "43\n",
      "\n",
      "13\n",
      "2\n",
      "\n",
      "10\n",
      "8\n",
      "\n",
      "73\n",
      "730\n",
      "\n",
      "50\n",
      "64\n",
      "\n",
      "989\n",
      "998\n",
      "\n",
      "10\n",
      "11\n",
      "\n",
      "4\n",
      "88\n",
      "\n",
      "1.8\n",
      "1.9\n",
      "\n",
      "4\n",
      "2\n",
      "\n",
      "48\n",
      "48.7\n",
      "\n",
      "3\n",
      "4\n",
      "\n",
      "2\n",
      "40\n",
      "\n",
      "2011\n",
      "2000\n",
      "\n",
      "300\n",
      "299\n",
      "\n",
      "45\n",
      "66\n",
      "\n",
      "2\n",
      "4\n",
      "\n",
      "480\n",
      "500\n",
      "\n",
      "3\n",
      "122\n",
      "\n",
      "2\n",
      "78\n",
      "\n",
      "5\n",
      "6\n",
      "\n",
      "5\n",
      "11\n",
      "\n",
      "99\n",
      "0.99\n",
      "\n",
      "40\n",
      "39\n",
      "\n",
      "6\n",
      "2\n",
      "\n",
      "25\n",
      "70\n",
      "\n",
      "31.31\n",
      "33\n",
      "\n",
      "4\n",
      "5\n",
      "\n",
      "11\n",
      "2\n",
      "\n",
      "800\n",
      "1200\n",
      "\n",
      "2011\n",
      "1\n",
      "\n",
      "273\n",
      "270\n",
      "\n",
      "100\n",
      "459\n",
      "\n",
      "1/3\n",
      "36\n",
      "\n",
      "36\n",
      "35\n",
      "\n",
      "3.12\n",
      "3\n",
      "\n",
      "2011\n",
      "2009\n",
      "\n",
      "9\n",
      "10\n",
      "\n",
      "8.3\n",
      "8\n",
      "\n",
      "1\n",
      "2\n",
      "\n",
      "1\n",
      "1.5\n",
      "\n",
      "1\n",
      "2\n",
      "\n",
      "5\n",
      "6\n",
      "\n",
      "40\n",
      "35\n",
      "\n",
      "2010\n",
      "20\n",
      "\n",
      "10\n",
      "11\n",
      "\n",
      "5.99\n",
      "6\n",
      "\n",
      "10\n",
      "5\n",
      "\n",
      "3,959\n",
      "700\n",
      "\n",
      "1\n",
      "1.9\n",
      "\n",
      "83\n",
      "84\n",
      "\n",
      "111\n",
      "100\n",
      "\n",
      "2009\n",
      "09\n",
      "\n",
      "1\n",
      "25\n",
      "\n",
      "0.5\n",
      "99.5\n",
      "\n",
      "2\n",
      "13\n",
      "\n",
      "2\n",
      "3\n",
      "\n",
      "1\n",
      "4\n",
      "\n",
      "2010\n",
      "10\n",
      "\n",
      "2012\n",
      "2013\n",
      "\n",
      "49\n",
      "50\n",
      "\n",
      "2012\n",
      "2009\n",
      "\n",
      "1\n",
      "1,000\n",
      "\n",
      "2013\n",
      "2014\n",
      "\n",
      "3\n",
      "4\n",
      "\n",
      "6.7\n",
      "7.6\n",
      "\n",
      "10.25\n",
      "10\n",
      "\n",
      "20\n",
      "10\n",
      "\n",
      "/\n",
      "1\n",
      "\n",
      "2025\n",
      "12\n",
      "\n",
      "1\n",
      "40\n",
      "\n",
      "9/11\n",
      "20\n",
      "\n",
      "2012\n",
      "7\n",
      "\n",
      "2\n",
      "30\n",
      "\n",
      "1970\n",
      "70\n",
      "\n",
      "9\n",
      "13\n",
      "\n",
      "58\n",
      "529\n",
      "\n",
      "129,864\n",
      "129,864,880\n",
      "\n",
      "1999\n",
      "2011\n",
      "\n",
      "20\n",
      "21\n",
      "\n",
      "8.855\n",
      "8.85\n",
      "\n",
      "98\n",
      "2008\n",
      "\n",
      "3\n",
      "14\n",
      "\n",
      "0.65\n",
      "65\n",
      "\n",
      "165\n",
      "124\n",
      "\n",
      "35\n",
      "75\n",
      "\n",
      "1.2\n",
      "1\n",
      "\n",
      "814\n",
      "815\n",
      "\n",
      "17.7\n",
      "18\n",
      "\n",
      "1961\n",
      "61\n",
      "\n",
      "5\n",
      "4\n",
      "\n",
      "10\n",
      "7\n",
      "\n",
      "3\n",
      "0\n",
      "\n",
      "862\n",
      "1\n",
      "\n",
      "1.2\n",
      "1\n",
      "\n",
      "4,700\n",
      "37.50\n",
      "\n",
      "96\n",
      "97\n",
      "\n",
      "3\n",
      "30\n",
      "\n",
      "1863\n",
      "1813\n",
      "\n",
      "74.5\n",
      "75\n",
      "\n",
      "239\n",
      "240\n",
      "\n",
      "10\n",
      "11\n",
      "\n",
      "889\n",
      "100\n",
      "\n",
      "53\n",
      "71\n",
      "\n",
      "10\n",
      "1\n",
      "\n",
      "23.9\n",
      "24\n",
      "\n",
      "4\n",
      "3\n",
      "\n",
      "2018\n",
      "2011\n",
      "\n",
      "6,000\n",
      "30\n",
      "\n",
      "2\n",
      "8\n",
      "\n",
      "1\n",
      "5.4\n",
      "\n",
      "5\n",
      "20\n",
      "\n",
      "700\n",
      "130\n",
      "\n",
      "18\n",
      "1\n",
      "\n",
      "25\n",
      "33\n",
      "\n",
      "2016\n",
      "16\n",
      "\n",
      "9\n",
      "10\n",
      "\n",
      "1700\n",
      "400\n",
      "\n",
      "5\n",
      "100\n",
      "\n",
      "7.1\n",
      "7\n",
      "\n",
      "4025\n",
      "911\n",
      "\n",
      "2\n",
      "3\n",
      "\n",
      "200\n",
      "216\n",
      "\n",
      "6\n",
      "43\n",
      "\n",
      "4\n",
      "5\n",
      "\n",
      "1\n",
      "6\n",
      "\n",
      "4\n",
      "5\n",
      "\n",
      "7\n",
      "7.8\n",
      "\n",
      "88\n",
      "38\n",
      "\n",
      "/\n",
      "1886\n",
      "\n",
      "3\n",
      "6\n",
      "\n",
      "1988\n",
      "1980\n",
      "\n",
      "9\n",
      "7\n",
      "\n",
      "5.1\n",
      "5\n",
      "\n",
      "1\n",
      "30\n",
      "\n",
      "237\n",
      "258\n",
      "\n",
      "5\n",
      "100\n",
      "\n",
      "377\n",
      "337\n",
      "\n",
      "2017\n",
      "2019\n",
      "\n",
      "900\n",
      "980\n",
      "\n",
      "/\n",
      "1\n",
      "\n",
      "1\n",
      "2028\n",
      "\n",
      "35\n",
      "5\n",
      "\n",
      "2020\n",
      "4\n",
      "\n",
      "115\n",
      "300\n",
      "\n",
      "70\n",
      "78\n",
      "\n",
      "1.3\n",
      "1\n",
      "\n",
      "13\n",
      "2020\n",
      "\n",
      "234\n",
      "230\n",
      "\n",
      "3\n",
      "3.5\n",
      "\n",
      "1\n",
      "2020\n",
      "\n",
      "5\n",
      "6\n",
      "\n",
      "93\n",
      "90\n",
      "\n",
      "20\n",
      "19\n",
      "\n",
      "4\n",
      "27.5\n",
      "\n",
      "2\n",
      "2.1\n",
      "\n",
      "2015\n",
      "40\n",
      "\n",
      "2.4\n",
      "2\n",
      "\n",
      "51\n",
      "55\n",
      "\n",
      "3\n",
      "20\n",
      "\n",
      "7\n",
      "7.7\n",
      "\n",
      "5\n",
      "2\n",
      "\n",
      "30\n",
      "28\n",
      "\n",
      "1744\n",
      "1774\n",
      "\n",
      "50\n",
      "63\n",
      "\n",
      "3\n",
      "70\n",
      "\n",
      "1.8\n",
      "2\n",
      "\n",
      "5\n",
      "10\n",
      "\n",
      "40\n",
      "50\n",
      "\n",
      "25\n",
      "2\n",
      "\n",
      "20.4\n",
      "20\n",
      "\n",
      "4\n",
      "5\n",
      "\n",
      "126\n",
      "8\n",
      "\n",
      "3.1\n",
      "3\n",
      "\n",
      "3,000\n",
      "2,979\n",
      "\n",
      "10\n",
      "9.4\n",
      "\n",
      "100\n",
      "102\n",
      "\n",
      "2016\n",
      "2015\n",
      "\n",
      "548\n",
      "399\n",
      "\n",
      "4.8\n",
      "5\n",
      "\n",
      "66\n",
      "60\n",
      "\n",
      "8.76\n",
      "9\n",
      "\n",
      "1977\n",
      "1976\n",
      "\n",
      "67\n",
      "33\n",
      "\n",
      "21\n",
      "21.6\n",
      "\n",
      "4\n",
      "5\n",
      "\n",
      "86\n",
      "860\n",
      "\n",
      "73\n",
      "730\n",
      "\n",
      "1\n",
      "3\n",
      "\n",
      "23\n",
      "21\n",
      "\n",
      "33.8\n",
      "33\n",
      "\n",
      "30\n",
      "50\n",
      "\n",
      "1990\n",
      "6\n",
      "\n",
      "43.9\n",
      "43\n",
      "\n",
      "1.1\n",
      "1\n",
      "\n",
      "7.25\n",
      "7\n",
      "\n",
      "3\n",
      "39\n",
      "\n",
      "30\n",
      "32\n",
      "\n",
      "9\n",
      "10\n",
      "\n",
      "1997\n",
      "1994\n",
      "\n",
      "50\n",
      "56\n",
      "\n",
      "230\n",
      "200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "false=0\n",
    "pred = []\n",
    "for x,y in zip(updated_cal, test_data['ans']):\n",
    "    ans = t5_predictions2(x, tokenizer, model2)\n",
    "    pred.append(ans)\n",
    "    if ans != y:\n",
    "        print(ans)\n",
    "        print(y)\n",
    "        print(\"\")\n",
    "        false += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "30861885",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "305"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b3e2b2bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8814152410575428"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1-false/len(test_data['ans'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f90fe0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TF (transformers)",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
